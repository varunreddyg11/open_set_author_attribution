{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_SLELZpK4dET",
        "outputId": "5310d462-e121-46e1-a26c-5a4ea09522ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4gz_vtwS4nwO",
        "outputId": "649bde9a-f007-4f09-e112-98af8b3b463f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SBiai6Ux4oXR",
        "outputId": "d4cbc787-f9d1-4c95-bbfd-feb6e344b0ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.24.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.5 MB 24.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.13.0)\n",
            "Collecting huggingface-hub<1.0,>=0.10.0\n",
            "  Downloading huggingface_hub-0.11.0-py3-none-any.whl (182 kB)\n",
            "\u001b[K     |████████████████████████████████| 182 kB 59.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 57.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.10.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.11.0 tokenizers-0.13.2 transformers-4.24.0\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ygeLM2Pj40Lv",
        "outputId": "a2d41146-0d37-4dbf-9781-42eebea0e957"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.15, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.15, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.15, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.15, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.15, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.15, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.15, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.15, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.15, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.15, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.15, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.15, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.15, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.15, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.15, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.15, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.15, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.15, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.15, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.15, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.15, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.15, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.15, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.15, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.15, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.15, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=101, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "from torch.utils.data import TensorDataset, random_split\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import RobertaTokenizer,BertTokenizer, AutoConfig, AutoModelForSequenceClassification, AutoTokenizer\n",
        "from transformers import (\n",
        "    AdamW,\n",
        "    get_linear_schedule_with_warmup\n",
        ")\n",
        "output_dir = '/content/drive/MyDrive/fewshot_data/Closed_Bert/model_save_closed_Set/'\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(output_dir+'tokenizer/')\n",
        "known_model = AutoModelForSequenceClassification.from_pretrained(output_dir)\n",
        "\n",
        "\n",
        "# Copy the model to the GPU.\n",
        "known_model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gVtfkVrD5rna",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de4d0ef9-8d1c-40f5-b854-060a26451e76"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.15, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.15, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.15, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.15, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.15, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.15, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.15, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.15, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.15, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.15, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.15, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.15, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.15, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.15, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.15, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.15, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.15, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.15, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.15, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.15, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.15, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.15, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.15, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.15, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.15, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.15, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=101, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "from torch.utils.data import TensorDataset, random_split\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import RobertaTokenizer,BertTokenizer, AutoConfig, AutoModelForSequenceClassification, AutoTokenizer\n",
        "from transformers import (\n",
        "    AdamW,\n",
        "    get_linear_schedule_with_warmup\n",
        ")\n",
        "\n",
        "\n",
        "output_dir = '/content/drive/MyDrive/fewshot_data/M6/'\n",
        "\n",
        "#tokenizer = AutoTokenizer.from_pretrained(output_dir+'tokenizer/')\n",
        "unknown_model = AutoModelForSequenceClassification.from_pretrained(output_dir)\n",
        "\n",
        "\n",
        "# Copy the model to the GPU.\n",
        "unknown_model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tELH_Yzb5rpw"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import regex as re\n",
        "import numpy as np\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/fewshot_data/ood_test_data_small.csv\")\n",
        "df = df[['real_label','review']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_-KYwxE05rsb"
      },
      "outputs": [],
      "source": [
        "arr_real_labels = [i for i in range(100,10000)]\n",
        "df['real_label'] = df['real_label'].replace(arr_real_labels,100)\n",
        "df=df.loc[df.real_label.values<=100]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nOkElgJ-5rux"
      },
      "outputs": [],
      "source": [
        "# Create sentence and label lists\n",
        "#\n",
        "labels = df.real_label.values\n",
        "sentence1 = df.review.values\n",
        "#sentence1=sentences\n",
        "#labels = dataset_filtered.real_label.values\n",
        "\n",
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "\n",
        "# For every sentence...\n",
        "for sent1 in zip(sentence1):\n",
        "    # `encode_plus` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    #   (5) Pad or truncate the sentence to `max_length`\n",
        "    #   (6) Create attention masks for [PAD] tokens.\n",
        "    #print(sent1)\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        sent1[0],# Sentence to encode.\n",
        "                        truncation=True,\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        max_length = 512,           # Pad & truncate all sentences.\n",
        "                        padding='max_length',\n",
        "                        return_attention_mask = True,   # Construct attn. masks.\n",
        "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                        return_overflowing_tokens=False,\n",
        "                   )\n",
        "    \n",
        "    # Add the encoded sentence to the list.    \n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    \n",
        "    # And its attention mask (simply differentiates padding from non-padding).\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "# Convert the lists into tensors.\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "labels = torch.tensor(labels)\n",
        "\n",
        "\n",
        "\n",
        "# Set the batch size.  \n",
        "batch_size = 16\n",
        "\n",
        "# Create the DataLoader.\n",
        "prediction_data = TensorDataset(input_ids, attention_masks, labels)\n",
        "prediction_sampler = SequentialSampler(prediction_data)\n",
        "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qndCxtj25rxt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fd13c69-7770-4ba5-8ba3-2001dab4c5f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicting labels for 20,000 test sentences...\n",
            "    DONE.\n"
          ]
        }
      ],
      "source": [
        "# Prediction on test set\n",
        "\n",
        "print('Predicting labels for {:,} test sentences...'.format(len(input_ids)))\n",
        "\n",
        "# Put model in evaluation mode\n",
        "known_model.eval()\n",
        "\n",
        "unknown_model.eval()\n",
        "\n",
        "unknown_logits_total_array = []\n",
        "known_logits_total_array = []\n",
        "\n",
        "# Tracking variables \n",
        "unknown_predictions , unknown_true_labels = [], []\n",
        "known_predictions , known_true_labels = [], []\n",
        "\n",
        "# Predict \n",
        "for batch in prediction_dataloader:\n",
        "  # Add batch to GPU\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "  \n",
        "  # Unpack the inputs from our dataloader\n",
        "  b_input_ids, b_input_mask, b_labels = batch\n",
        "  \n",
        "  # Telling the model not to compute or store gradients, saving memory and \n",
        "  # speeding up prediction\n",
        "  with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions.\n",
        "      unknown_result = unknown_model(b_input_ids, \n",
        "                     token_type_ids=None, \n",
        "                     attention_mask=b_input_mask,\n",
        "                     return_dict=True)\n",
        "      known_result = known_model(b_input_ids, \n",
        "                     token_type_ids=None, \n",
        "                     attention_mask=b_input_mask,\n",
        "                     return_dict=True)\n",
        "\n",
        "  unknown_logits = unknown_result.logits\n",
        "  known_logits = known_result.logits\n",
        "  # Move logits and labels to CPU\n",
        "  unknown_logits = unknown_logits.detach().cpu().numpy()\n",
        "  unknown_logits[:,100] = unknown_logits[:,100] + 11\n",
        "  unknown_logits_total_array.extend(unknown_logits.tolist())\n",
        "  unknown_pred_labels = np.argmax(unknown_logits, axis=1)\n",
        "  unknown_label_ids = b_labels.to('cpu').numpy()\n",
        "  \n",
        "  # Store predictions and true labels\n",
        "  unknown_predictions.extend(unknown_pred_labels.tolist())\n",
        "  unknown_true_labels.extend(unknown_label_ids.tolist())\n",
        "\n",
        "  # Move logits and labels to CPU\n",
        "  known_logits = known_logits.detach().cpu().numpy()\n",
        "  known_logits[:,100] = known_logits[:,100] + 0\n",
        "  known_logits_total_array.extend(known_logits.tolist())\n",
        "  known_pred_labels = np.argmax(known_logits, axis=1)\n",
        "  known_label_ids = b_labels.to('cpu').numpy()\n",
        "  \n",
        "  # Store predictions and true labels\n",
        "  known_predictions.extend(known_pred_labels.tolist())\n",
        "  known_true_labels.extend(known_label_ids.tolist())\n",
        "\n",
        "print('    DONE.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UJTnWVWI5r08",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "894cc61c-9888-4a17-aad7-05f5e10e582c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0    0.75472   0.80000   0.77670       100\n",
            "           1    0.50000   0.01000   0.01961       100\n",
            "           2    0.94118   0.64000   0.76190       100\n",
            "           3    0.94898   0.93000   0.93939       100\n",
            "           4    0.63333   0.19000   0.29231       100\n",
            "           5    0.82759   0.48000   0.60759       100\n",
            "           6    0.73134   0.49000   0.58683       100\n",
            "           7    1.00000   0.72000   0.83721       100\n",
            "           8    0.62000   0.93000   0.74400       100\n",
            "           9    0.72619   0.61000   0.66304       100\n",
            "          10    0.91262   0.94000   0.92611       100\n",
            "          11    0.61538   0.80000   0.69565       100\n",
            "          12    0.58750   0.94000   0.72308       100\n",
            "          13    0.87234   0.41000   0.55782       100\n",
            "          14    0.66667   0.54000   0.59669       100\n",
            "          15    0.78151   0.93000   0.84932       100\n",
            "          16    0.97436   0.76000   0.85393       100\n",
            "          17    0.90722   0.88000   0.89340       100\n",
            "          18    0.88372   0.76000   0.81720       100\n",
            "          19    0.66964   0.75000   0.70755       100\n",
            "          20    0.00000   0.00000   0.00000       100\n",
            "          21    0.60448   0.81000   0.69231       100\n",
            "          22    0.55882   0.57000   0.56436       100\n",
            "          23    0.67391   0.62000   0.64583       100\n",
            "          24    0.78261   0.54000   0.63905       100\n",
            "          25    0.83168   0.84000   0.83582       100\n",
            "          26    0.97403   0.75000   0.84746       100\n",
            "          27    0.70833   0.51000   0.59302       100\n",
            "          28    0.60870   0.42000   0.49704       100\n",
            "          29    0.80357   0.90000   0.84906       100\n",
            "          30    0.75893   0.85000   0.80189       100\n",
            "          31    0.90476   0.76000   0.82609       100\n",
            "          32    0.43455   0.83000   0.57045       100\n",
            "          33    0.68293   0.28000   0.39716       100\n",
            "          34    0.85714   0.60000   0.70588       100\n",
            "          35    0.82857   0.29000   0.42963       100\n",
            "          36    0.88636   0.39000   0.54167       100\n",
            "          37    1.00000   0.02000   0.03922       100\n",
            "          38    1.00000   0.59000   0.74214       100\n",
            "          39    0.74747   0.74000   0.74372       100\n",
            "          40    0.52475   0.53000   0.52736       100\n",
            "          41    0.72000   0.90000   0.80000       100\n",
            "          42    0.96923   0.63000   0.76364       100\n",
            "          43    0.66667   0.52000   0.58427       100\n",
            "          44    0.75000   0.57000   0.64773       100\n",
            "          45    1.00000   0.23000   0.37398       100\n",
            "          46    0.00000   0.00000   0.00000       100\n",
            "          47    0.00000   0.00000   0.00000       100\n",
            "          48    0.87234   0.82000   0.84536       100\n",
            "          49    0.96629   0.86000   0.91005       100\n",
            "          50    0.71667   0.43000   0.53750       100\n",
            "          51    0.97561   0.40000   0.56738       100\n",
            "          52    0.54902   0.84000   0.66403       100\n",
            "          53    0.48750   0.78000   0.60000       100\n",
            "          54    0.84946   0.79000   0.81865       100\n",
            "          55    0.35745   0.84000   0.50149       100\n",
            "          56    0.96000   0.96000   0.96000       100\n",
            "          57    0.89583   0.86000   0.87755       100\n",
            "          58    0.88764   0.79000   0.83598       100\n",
            "          59    0.00000   0.00000   0.00000       100\n",
            "          60    0.77193   0.88000   0.82243       100\n",
            "          61    0.70833   0.68000   0.69388       100\n",
            "          62    0.70312   0.90000   0.78947       100\n",
            "          63    0.90909   0.80000   0.85106       100\n",
            "          64    0.89474   0.85000   0.87179       100\n",
            "          65    0.93827   0.76000   0.83978       100\n",
            "          66    1.00000   0.07000   0.13084       100\n",
            "          67    0.83871   0.78000   0.80829       100\n",
            "          68    0.62857   0.22000   0.32593       100\n",
            "          69    0.75000   0.12000   0.20690       100\n",
            "          70    0.81481   0.22000   0.34646       100\n",
            "          71    0.00000   0.00000   0.00000       100\n",
            "          72    0.75758   1.00000   0.86207       100\n",
            "          73    0.59813   0.64000   0.61836       100\n",
            "          74    0.00000   0.00000   0.00000       100\n",
            "          75    0.95775   0.68000   0.79532       100\n",
            "          76    0.66667   0.02000   0.03883       100\n",
            "          77    0.78947   0.75000   0.76923       100\n",
            "          78    0.65812   0.77000   0.70968       100\n",
            "          79    0.00000   0.00000   0.00000       100\n",
            "          80    0.00000   0.00000   0.00000       100\n",
            "          81    0.49457   0.91000   0.64085       100\n",
            "          82    0.78313   0.65000   0.71038       100\n",
            "          83    0.97030   0.98000   0.97512       100\n",
            "          84    0.92188   0.59000   0.71951       100\n",
            "          85    0.51948   0.80000   0.62992       100\n",
            "          86    0.74336   0.84000   0.78873       100\n",
            "          87    0.90909   0.10000   0.18018       100\n",
            "          88    0.00000   0.00000   0.00000       100\n",
            "          89    0.68571   0.72000   0.70244       100\n",
            "          90    0.59664   0.71000   0.64840       100\n",
            "          91    0.82500   0.33000   0.47143       100\n",
            "          92    0.00000   0.00000   0.00000       100\n",
            "          93    1.00000   0.78000   0.87640       100\n",
            "          94    0.00000   0.00000   0.00000       100\n",
            "          95    1.00000   0.13000   0.23009       100\n",
            "          96    0.83036   0.93000   0.87736       100\n",
            "          97    0.89189   0.33000   0.48175       100\n",
            "          98    0.70115   0.61000   0.65241       100\n",
            "          99    0.85000   0.68000   0.75556       100\n",
            "         100    0.67998   0.83760   0.75060     10000\n",
            "\n",
            "    accuracy                        0.70280     20000\n",
            "   macro avg    0.69443   0.57067   0.58611     20000\n",
            "weighted avg    0.68728   0.70280   0.66754     20000\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0    0.42211   0.84000   0.56187       100\n",
            "           1    0.75833   0.91000   0.82727       100\n",
            "           2    0.58000   0.87000   0.69600       100\n",
            "           3    0.70504   0.98000   0.82008       100\n",
            "           4    0.15222   0.65000   0.24668       100\n",
            "           5    0.28716   0.85000   0.42929       100\n",
            "           6    0.39474   0.60000   0.47619       100\n",
            "           7    0.69014   0.98000   0.80992       100\n",
            "           8    0.49735   0.94000   0.65052       100\n",
            "           9    0.45361   0.88000   0.59864       100\n",
            "          10    0.70149   0.94000   0.80342       100\n",
            "          11    0.35745   0.84000   0.50149       100\n",
            "          12    0.36863   0.94000   0.52958       100\n",
            "          13    0.63946   0.94000   0.76113       100\n",
            "          14    0.41279   0.71000   0.52206       100\n",
            "          15    0.53591   0.97000   0.69039       100\n",
            "          16    0.80531   0.91000   0.85446       100\n",
            "          17    0.62759   0.91000   0.74286       100\n",
            "          18    0.62500   0.90000   0.73770       100\n",
            "          19    0.35217   0.81000   0.49091       100\n",
            "          20    0.90385   0.94000   0.92157       100\n",
            "          21    0.43878   0.86000   0.58108       100\n",
            "          22    0.43796   0.60000   0.50633       100\n",
            "          23    0.28723   0.81000   0.42408       100\n",
            "          24    0.33191   0.78000   0.46567       100\n",
            "          25    0.40807   0.91000   0.56347       100\n",
            "          26    0.56897   0.99000   0.72263       100\n",
            "          27    0.29902   0.61000   0.40132       100\n",
            "          28    0.36508   0.92000   0.52273       100\n",
            "          29    0.58537   0.96000   0.72727       100\n",
            "          30    0.45026   0.86000   0.59107       100\n",
            "          31    0.59574   0.84000   0.69710       100\n",
            "          32    0.19400   0.97000   0.32333       100\n",
            "          33    0.29630   0.56000   0.38754       100\n",
            "          34    0.46821   0.81000   0.59341       100\n",
            "          35    0.41748   0.43000   0.42365       100\n",
            "          36    0.48295   0.85000   0.61594       100\n",
            "          37    0.53631   0.96000   0.68817       100\n",
            "          38    0.76800   0.96000   0.85333       100\n",
            "          39    0.24936   0.98000   0.39757       100\n",
            "          40    0.23641   0.87000   0.37179       100\n",
            "          41    0.44344   0.98000   0.61059       100\n",
            "          42    0.68142   0.77000   0.72300       100\n",
            "          43    0.32843   0.67000   0.44079       100\n",
            "          44    0.47468   0.75000   0.58140       100\n",
            "          45    0.88119   0.89000   0.88557       100\n",
            "          46    0.62112   1.00000   0.76628       100\n",
            "          47    0.27515   0.93000   0.42466       100\n",
            "          48    0.63333   0.95000   0.76000       100\n",
            "          49    0.63816   0.97000   0.76984       100\n",
            "          50    0.29268   0.84000   0.43411       100\n",
            "          51    0.71429   0.85000   0.77626       100\n",
            "          52    0.39910   0.89000   0.55108       100\n",
            "          53    0.23641   0.87000   0.37179       100\n",
            "          54    0.72881   0.86000   0.78899       100\n",
            "          55    0.26667   0.92000   0.41348       100\n",
            "          56    0.47343   0.98000   0.63844       100\n",
            "          57    0.37549   0.95000   0.53824       100\n",
            "          58    0.60000   0.90000   0.72000       100\n",
            "          59    0.38956   0.97000   0.55587       100\n",
            "          60    0.44670   0.88000   0.59259       100\n",
            "          61    0.27817   0.79000   0.41146       100\n",
            "          62    0.41176   0.91000   0.56698       100\n",
            "          63    0.76786   0.86000   0.81132       100\n",
            "          64    0.85714   0.84000   0.84848       100\n",
            "          65    0.60544   0.89000   0.72065       100\n",
            "          66    0.59477   0.91000   0.71937       100\n",
            "          67    0.62121   0.82000   0.70690       100\n",
            "          68    0.20502   0.49000   0.28909       100\n",
            "          69    0.45506   0.81000   0.58273       100\n",
            "          70    0.36866   0.80000   0.50473       100\n",
            "          71    0.34783   0.72000   0.46906       100\n",
            "          72    0.48544   1.00000   0.65359       100\n",
            "          73    0.34632   0.80000   0.48338       100\n",
            "          74    0.68148   0.92000   0.78298       100\n",
            "          75    0.95062   0.77000   0.85083       100\n",
            "          76    0.65306   0.96000   0.77733       100\n",
            "          77    0.55333   0.83000   0.66400       100\n",
            "          78    0.32103   0.87000   0.46900       100\n",
            "          79    0.90741   0.98000   0.94231       100\n",
            "          80    0.55866   1.00000   0.71685       100\n",
            "          81    0.24798   0.92000   0.39066       100\n",
            "          82    0.38428   0.88000   0.53495       100\n",
            "          83    0.78125   1.00000   0.87719       100\n",
            "          84    0.61069   0.80000   0.69264       100\n",
            "          85    0.35849   0.95000   0.52055       100\n",
            "          86    0.21212   0.98000   0.34875       100\n",
            "          87    0.40217   0.74000   0.52113       100\n",
            "          88    0.33333   0.79000   0.46884       100\n",
            "          89    0.24545   0.81000   0.37674       100\n",
            "          90    0.35102   0.86000   0.49855       100\n",
            "          91    0.62903   0.78000   0.69643       100\n",
            "          92    0.33562   0.98000   0.50000       100\n",
            "          93    0.79091   0.87000   0.82857       100\n",
            "          94    0.37052   0.93000   0.52991       100\n",
            "          95    0.86813   0.79000   0.82723       100\n",
            "          96    0.49223   0.95000   0.64846       100\n",
            "          97    0.51799   0.72000   0.60251       100\n",
            "          98    0.57229   0.95000   0.71429       100\n",
            "          99    0.54438   0.92000   0.68401       100\n",
            "         100    0.00000   0.00000   0.00000     10000\n",
            "\n",
            "    accuracy                        0.43125     20000\n",
            "   macro avg    0.49115   0.85396   0.60500     20000\n",
            "weighted avg    0.24803   0.43125   0.30552     20000\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "unknown_result_report= classification_report(unknown_true_labels, unknown_predictions, digits=5)\n",
        "print(unknown_result_report)\n",
        "\n",
        "known_result_report= classification_report(known_true_labels, known_predictions, digits=5)\n",
        "print(known_result_report)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions=[]\n",
        "for i in range(len(unknown_true_labels)):\n",
        "  if unknown_predictions[i]==100:\n",
        "    predictions.append(100)\n",
        "  else :\n",
        "    predictions.append(known_predictions[i])\n",
        "\n",
        "\n",
        "result_report= classification_report(known_true_labels, predictions, digits=5)\n",
        "print(result_report)"
      ],
      "metadata": {
        "id": "_JrWGjL6B2Bn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fcf4ef0-d8ff-4ec0-f16f-c0ac1cfadf40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0    0.68376   0.80000   0.73733       100\n",
            "           1    1.00000   0.01000   0.01980       100\n",
            "           2    0.92958   0.66000   0.77193       100\n",
            "           3    0.93939   0.93000   0.93467       100\n",
            "           4    0.39130   0.18000   0.24658       100\n",
            "           5    0.60241   0.50000   0.54645       100\n",
            "           6    0.73770   0.45000   0.55901       100\n",
            "           7    0.91026   0.71000   0.79775       100\n",
            "           8    0.73228   0.93000   0.81938       100\n",
            "           9    0.72289   0.60000   0.65574       100\n",
            "          10    0.92157   0.94000   0.93069       100\n",
            "          11    0.58647   0.78000   0.66953       100\n",
            "          12    0.63448   0.92000   0.75102       100\n",
            "          13    0.78431   0.40000   0.52980       100\n",
            "          14    0.67059   0.57000   0.61622       100\n",
            "          15    0.86111   0.93000   0.89423       100\n",
            "          16    0.98684   0.75000   0.85227       100\n",
            "          17    0.87755   0.86000   0.86869       100\n",
            "          18    0.88372   0.76000   0.81720       100\n",
            "          19    0.70707   0.70000   0.70352       100\n",
            "          20    0.00000   0.00000   0.00000       100\n",
            "          21    0.68421   0.78000   0.72897       100\n",
            "          22    0.64198   0.52000   0.57459       100\n",
            "          23    0.59259   0.64000   0.61538       100\n",
            "          24    0.61290   0.57000   0.59067       100\n",
            "          25    0.89362   0.84000   0.86598       100\n",
            "          26    0.89286   0.75000   0.81522       100\n",
            "          27    0.58571   0.41000   0.48235       100\n",
            "          28    0.58571   0.41000   0.48235       100\n",
            "          29    0.80357   0.90000   0.84906       100\n",
            "          30    0.76852   0.83000   0.79808       100\n",
            "          31    0.90588   0.77000   0.83243       100\n",
            "          32    0.45902   0.84000   0.59364       100\n",
            "          33    0.71795   0.28000   0.40288       100\n",
            "          34    0.84000   0.63000   0.72000       100\n",
            "          35    0.83871   0.26000   0.39695       100\n",
            "          36    0.86364   0.38000   0.52778       100\n",
            "          37    0.30000   0.03000   0.05455       100\n",
            "          38    0.98333   0.59000   0.73750       100\n",
            "          39    0.66087   0.76000   0.70698       100\n",
            "          40    0.48214   0.54000   0.50943       100\n",
            "          41    0.73554   0.89000   0.80543       100\n",
            "          42    0.95385   0.62000   0.75152       100\n",
            "          43    0.64198   0.52000   0.57459       100\n",
            "          44    0.74026   0.57000   0.64407       100\n",
            "          45    1.00000   0.26000   0.41270       100\n",
            "          46    0.00000   0.00000   0.00000       100\n",
            "          47    0.00000   0.00000   0.00000       100\n",
            "          48    0.86170   0.81000   0.83505       100\n",
            "          49    0.93478   0.86000   0.89583       100\n",
            "          50    0.62687   0.42000   0.50299       100\n",
            "          51    0.97500   0.39000   0.55714       100\n",
            "          52    0.61194   0.82000   0.70085       100\n",
            "          53    0.48701   0.75000   0.59055       100\n",
            "          54    0.90805   0.79000   0.84492       100\n",
            "          55    0.47977   0.83000   0.60806       100\n",
            "          56    0.89720   0.96000   0.92754       100\n",
            "          57    0.81905   0.86000   0.83902       100\n",
            "          58    0.88506   0.77000   0.82353       100\n",
            "          59    0.20000   0.01000   0.01905       100\n",
            "          60    0.74336   0.84000   0.78873       100\n",
            "          61    0.63551   0.68000   0.65700       100\n",
            "          62    0.72358   0.89000   0.79821       100\n",
            "          63    0.94118   0.80000   0.86486       100\n",
            "          64    0.92222   0.83000   0.87368       100\n",
            "          65    0.92593   0.75000   0.82873       100\n",
            "          66    0.57143   0.08000   0.14035       100\n",
            "          67    0.84884   0.73000   0.78495       100\n",
            "          68    0.52500   0.21000   0.30000       100\n",
            "          69    0.64000   0.16000   0.25600       100\n",
            "          70    0.84615   0.22000   0.34921       100\n",
            "          71    0.00000   0.00000   0.00000       100\n",
            "          72    0.85470   1.00000   0.92166       100\n",
            "          73    0.67442   0.58000   0.62366       100\n",
            "          74    0.00000   0.00000   0.00000       100\n",
            "          75    1.00000   0.68000   0.80952       100\n",
            "          76    0.66667   0.02000   0.03883       100\n",
            "          77    0.80682   0.71000   0.75532       100\n",
            "          78    0.64463   0.78000   0.70588       100\n",
            "          79    0.00000   0.00000   0.00000       100\n",
            "          80    0.00000   0.00000   0.00000       100\n",
            "          81    0.51744   0.89000   0.65441       100\n",
            "          82    0.75000   0.66000   0.70213       100\n",
            "          83    0.95146   0.98000   0.96552       100\n",
            "          84    0.87879   0.58000   0.69880       100\n",
            "          85    0.64754   0.79000   0.71171       100\n",
            "          86    0.57534   0.84000   0.68293       100\n",
            "          87    0.73333   0.11000   0.19130       100\n",
            "          88    0.00000   0.00000   0.00000       100\n",
            "          89    0.60938   0.78000   0.68421       100\n",
            "          90    0.61345   0.73000   0.66667       100\n",
            "          91    0.82500   0.33000   0.47143       100\n",
            "          92    0.00000   0.00000   0.00000       100\n",
            "          93    0.95122   0.78000   0.85714       100\n",
            "          94    0.00000   0.00000   0.00000       100\n",
            "          95    0.87500   0.14000   0.24138       100\n",
            "          96    0.83784   0.93000   0.88152       100\n",
            "          97    0.83721   0.36000   0.50350       100\n",
            "          98    0.72289   0.60000   0.65574       100\n",
            "          99    0.86076   0.68000   0.75978       100\n",
            "         100    0.67998   0.83760   0.75060     10000\n",
            "\n",
            "    accuracy                        0.70070     20000\n",
            "   macro avg    0.67596   0.56651   0.58331     20000\n",
            "weighted avg    0.67795   0.70070   0.66612     20000\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tmj2fw2u6n2H"
      },
      "outputs": [],
      "source": [
        "df2 = pd.read_csv(\"/content/drive/MyDrive/fewshot_data/ood_test_data_small.csv\")\n",
        "df2[\"N_plus_one_prediction\"] = predictions\n",
        "df2['N_plus_one_prediction'] = df2['N_plus_one_prediction'].replace([100], 'OOD')\n",
        "combined_df = df2\n",
        "number_of_known_labels = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JYLWdesn6n7B"
      },
      "outputs": [],
      "source": [
        "def compute_precision_recall_for_known_classes(given_label):\n",
        "    TP = combined_df[(combined_df[\"real_label\"] == given_label) & (combined_df[\"N_plus_one_prediction\"] == given_label)].shape[0]\n",
        "    FP = combined_df[(combined_df[\"real_label\"] != given_label) & (combined_df[\"N_plus_one_prediction\"] == given_label)].shape[0]\n",
        "    FN = combined_df[(combined_df[\"real_label\"] == given_label) & (combined_df[\"N_plus_one_prediction\"] != given_label)].shape[0]\n",
        "    if(TP == 0):\n",
        "        P = 0\n",
        "        R = 0\n",
        "    else:\n",
        "        P = round(100*TP/(TP + FP), 2)\n",
        "        R = round(100*TP/(TP + FN),2)\n",
        "    return P, R\n",
        "\n",
        "def compute_precision_recall_for_OOD():\n",
        "    TP = combined_df[(combined_df[\"real_label\"] >= number_of_known_labels) & (combined_df[\"N_plus_one_prediction\"] == \"OOD\")].shape[0]\n",
        "    FP = combined_df[(combined_df[\"real_label\"] < number_of_known_labels) & (combined_df[\"N_plus_one_prediction\"] == \"OOD\")].shape[0]\n",
        "    FN = combined_df[(combined_df[\"real_label\"] >= number_of_known_labels) & (combined_df[\"N_plus_one_prediction\"] != \"OOD\")].shape[0]\n",
        "    if(TP == 0):\n",
        "        P = 0\n",
        "        R = 0\n",
        "    else:\n",
        "        P = 100*TP/(TP + FP)\n",
        "        R = 100*TP/(TP + FN)\n",
        "    return P, R\n",
        "\n",
        "def compute_performance_metrics():\n",
        "    from statistics import mean\n",
        "    \n",
        "    precisions = []\n",
        "    recalls = []\n",
        "    for known_label in range(number_of_known_labels):\n",
        "        p, r = compute_precision_recall_for_known_classes(known_label)\n",
        "        precisions.append(p)\n",
        "        recalls.append(r)\n",
        "    \n",
        "    precision_known, recall_known = round(mean(precisions),2), round(mean(recalls),2)\n",
        "    f1_known = round(2*precision_known*recall_known/(precision_known + recall_known),2)\n",
        "    \n",
        "    precision_ood, recall_ood = compute_precision_recall_for_OOD()\n",
        "    f1_ood = round(2*precision_ood*recall_ood/(precision_ood + recall_ood),2)\n",
        "    \n",
        "    precisions.append(precision_ood)\n",
        "    recalls.append(recall_ood)\n",
        "    \n",
        "    precision, recall = round(mean(precisions),2), round(mean(recalls),2)\n",
        "    f1 = round(2*precision*recall/(precision + recall),2)\n",
        "    \n",
        "    return {\n",
        "        \"known\": {\n",
        "            \"precision\": precision_known,\n",
        "            \"recall\": recall_known,\n",
        "            \"f1\": f1_known,\n",
        "        },\n",
        "        \"ood\": {\n",
        "            \"precision\": round(precision_ood,2),\n",
        "            \"recall\": round(recall_ood,2),\n",
        "            \"f1\": f1_ood,\n",
        "        },\n",
        "        \"overall\": {\n",
        "            \"precision\": precision,\n",
        "            \"recall\": recall,\n",
        "            \"f1\": f1,\n",
        "        }\n",
        "        \n",
        "    }\n",
        "\n",
        "def print_performance_metrics():\n",
        "    performance_dict = compute_performance_metrics()\n",
        "    print(\"Precision  Recall f1\")\n",
        "    print(\"Known:\",performance_dict[\"known\"][\"precision\"],performance_dict[\"known\"][\"recall\"],performance_dict[\"known\"][\"f1\"], end = \" \\n\")\n",
        "    print(\"odd:\",performance_dict[\"ood\"][\"precision\"],performance_dict[\"ood\"][\"recall\"],performance_dict[\"ood\"][\"f1\"], end = \" \\n\")\n",
        "    print(\"overall:\",performance_dict[\"overall\"][\"precision\"],performance_dict[\"overall\"][\"recall\"],performance_dict[\"overall\"][\"f1\"], end = \" \\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Epm5OwGY6oFi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd7ac1f1-57ec-478b-94c1-3f60664ce848"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision  Recall f1\n",
            "Known: 67.59 56.38 61.48 \n",
            "odd: 68.0 83.76 75.06 \n",
            "overall: 67.6 56.65 61.64 \n"
          ]
        }
      ],
      "source": [
        "print_performance_metrics()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t3erSlxeTm-w"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}